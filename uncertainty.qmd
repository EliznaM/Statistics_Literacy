# Uncertainty



Statistics is the study of uncertainty. Uncertainty is the source of variability in data and variability is what makes data interesting. In a given situation, there are many sources of variability. Some of these are real effects that we want to measure, while others are just noise that we want to ignore. The goal of statistical analysis is to separate the signal from the noise, to identify the real effects and to quantify their uncertainty. Our challenge is that it is not clear which parts of the data are signal and which parts are noise. 

If we were always measuring the same thing in the same way, like one would in pure physics experiments, for example, we could just repeat the measurements many times and average them to get a good estimate of the true value. Because that is what we are in fact seeking - the true value of the thing we are measuring. However, in most real-world situations, especially in the life sciences, just repeating the measurements many times, doesn't fully solve the problem. Biological systems vary even from minute to minute, and our tools for measuring also introduce variability because of characteristics inherent to the technology.  

Imagine measuring a certain concentration of a protein in blood samples from different individuals. The concentration of the protein may vary between individuals due to genetic differences, environmental factors, or health status. Additionally, the measurement within the same person will vary over time.  Even taking the measurement from the same blood sample, but at two different instances, on the same instrument will vary because of changes in temperature, air pressure and the calibration of instruments slowly changing over time. Two different lab members measuring the same sample may get slightly different results. All these sources of variability contribute to the uncertainty in our measurements.

We know that uncertainty will be there, but there are better and worse scenarios. The best case scenario is if the variability is random and unbiased. This means that the errors are equally likely to be positive or negative, and they do not systematically favor one direction over another. In this case, we can use statistical methods to estimate the true value and quantify the uncertainty around it.  

Another scenario that is potentially easier to manage, is if the error is systematic and consistent. If we know that a certain instrument always overestimates the concentration by 5%, we can correct for that bias in our analysis. One of the worst scenarios is if the variability is both systematic and inconsistent. And add to that the possibility that we are unaware of the systematic errors, then we are in trouble. 

For systematic errors, we often have to investigate and correct it in the study design or the technology used for measurement. Statistics in general can not solve systematic errors easily, and it is unwise to design a study in a sloppy way, hoping that statistical analysis will fix it later. Statistics is however very good at handling random errors, and many times in fact, rely on the assumption that errors are random and unbiased. 

Random error can be small or large, depending on the situation. Sometimes, we can control the sources of variability to some extent, for example by standardizing the measurement procedures, using high-quality instruments, training personnel or choosing to study a homogeneous population, for example in-bred lab animals. These approaches try to ensure that there is less unnecessary noise in the data, which would drown out the signal we are looking for. Some situations are however, inherently more variable. Some technologies are more noisy than others, because a better way to measure just has not been developed yet. It is evident that studying human populations will be more variable than studying in-bred lab animals.

With statistics we attept to quantify how variable the system is. Many times, this takes the form of providing some sort of range around the estimate we are providing. For example, instead of saying that the concentration of a protein is 50 ng/mL, we might say that it is 50 ng/mL +/- 5 ng/mL, indicating that the true value is likely to be between 45 and 55 ng/mL. With this we acknowledge the uncertainty in our measurement and provide a more informative result for others to judge. In some situations, the stakes are high, because the estimate may change clinical practice or influence public health decisions. In these cases, it is especially important to quantify and communicate the uncertainty in the estimates.  


