# The sampling distribution Part 1

```{r}
library(tidyverse)
library(knitr)
library(kableExtra)

themeCols <- tibble(cols = c("grey", "orange", "green"),
                    hex = c("#5F6062", "#C98C53", "#C9DCB3"))

theme_set(theme_minimal())

```

::: {.callout-tip collapse="false" appearance="simple"}
## Outcomes

  * Explain how a sampling distribution is constructed from sample data  
  * Explain what a probability distribution is  
  * Explain area-under-the-curve  
  * Understand the conceptual link between a sampling distribution from data and theoretical distributions like the normal distribution and t-distribution  
  * Understand the central limit theorem conceptually  

:::

Imagine you are involved in a field study where a specific species of frog is caught, weighed, and released again, with the goal of estimating the mean weight of the species.  You generate data that looks like the table below. 


```{r}

tibble(id = c("f1", "f2", "f3", "f4"),
       weight = c(33, 30, 34, 36)) |> 
  kable(table.attr = 'data-quarto-disable-processing="true"') |>
  kable_styling(full_width = FALSE, position = "left")
  
```

You do this not for only 4 frogs, like in the table, but for 40 frogs in total.  You calculate the mean weight of all the frogs.

```{r}

tibble(calculation = c("mean weight"),
       mean = c(value = 33)) |> 
  kable(table.attr = 'data-quarto-disable-processing="true"') |>
  kable_styling(full_width = FALSE, position = "left")
  

```

Now imagine doing this same study, 1000 times, with a different 40 frogs each time.  Of course you cannot actually do this in real life (it would take a lot of time and money), but we are doing a *thought experiment*.  For each of the studies, you calculate the mean weight of the frogs.  Then you draw a histogram of all 1000 mean weights that you calculated and get the plot below.


```{r}
set.seed(345)
plotdta <- tibble(mean_wt = rnorm(1000, mean = 34, sd = 3.5),
       mean_pop = mean(mean_wt),
       q2.5 = quantile(mean_wt, probs = 0.025),
       q97.5 = quantile(mean_wt, probs = 0.975)) 

plotdta |> 
  ggplot(aes(mean_wt))+
  geom_histogram()+
  geom_vline(xintercept = plotdta$mean_pop, colour = "black")+
  geom_vline(xintercept = plotdta$q2.5, linetype = 2, colour = "grey70")+
  geom_vline(xintercept = plotdta$q97.5, linetype = 2, colour = "grey70")+
  annotate("text", x = plotdta$mean_pop, 
           y = 100, label = round(plotdta$mean_pop, 2))+
  annotate("text", x = plotdta$q2.5, 
           y = 100, label = round(plotdta$q2.5, 2))+
  annotate("text", x = plotdta$q97.5, 
           y = 100, label = round(plotdta$q97.5, 2))


```
The mean of the 1000 means you calculated, is indicated by the vertical black line in the middle.  The 2.5th percentile and the 97.5th percentile are indicated by the grey dotted vertical lines.   
This distribution is the sampling distribution.  It is the distribution created by the 1000 different *samples* in our thought experiment.  

Recall what we discussed before - our experiment takes a *sample*, but our scientific question is actually about a *population* (all the individuals with the specific characteristics we are studying, e.g. all frogs of this species, or all tuberculosis patients in the world, or all HIV cases with suppressed viral loads in Sub-Saharan Africa, etc.)  So the sampling distribution is an attempt to represent the whole population, rather than just the 1 sample of 40 frogs that we have.  

But of course we have a problem - we were doing a *thought experiment* and our data and sampling distribution exists only in our thoughts, not in the real world.  Now we will introduce a new concept:  *resampling*.  

We already established that it is not possible for us to do 1000 of these frog experiments, but we can probably do at least one. If we have data for one experiment, we can apply a clever trick to pretend we still have 1000 experiments.  We do this by *resampling* from the data that we have.  

```{r}

n <- 40
r <- 30

x <- factorial(n)/(factorial(r)*factorial(n-r))

```

We take 1000 different random samples of frogs from the data that we generated by our one study.  If we want every new sample we take to contain 40 frogs, then we will take the same sample 1000 times, because we have only 40 frogs!  That is not helpful at all, because we will get exactly the same mean weight every time and the histogram above will have no spread at all.  We can follow one of two strategies:  

  1.  We must either take smaller samples, e.g 30 frogs, then if we take every possible combination of 30 frogs, from the pool of 40 frogs, we can theoretically get `r x` unique samples.  This type of resampling is called *resampling without replacement*.
  
::: {.column-margin}

The R code for the number of theoretical samples is:  

```{r}
#| echo: true

n <- 40
r <- 30

x <- factorial(n)/(factorial(r)*factorial(n-r))

```

:::

  2.  We take 40 frogs per sample, but we allow a frog to be picked for the sample, more than once, or not at all.  So the frog with ID `f1` might be picked for our first sample three times, while `f2` was picked only once, `f3` not picked at all and `f4`, once.  Then for our second sample, `f1` was picked once, `f2` twice, `f3` twice and `f4`, not at all.  And so on, until we have 1000 random samples.  This type of resampling is called *resampling with replacement*.  The idea is that all the frog IDs have been written on slips of paper, and put into a bag.  We draw out slips of paper, 1 at a time, write down the frog ID, then place the slip of paper back into the bag (instead of setting it aside so that it cannot be drawn out again). So the slip of paper can be drawn again, for the same sample.   
  
```{r}

tibble(sample_1 = c("f1", "f1", "f1", "f2", "f4", "..."),
       sample_2 = c("f1", "f2", "f2", "f3", "f3", "...")) |> 
  kable(table.attr = 'data-quarto-disable-processing="true"') |>
  kable_styling(full_width = FALSE, position = "left")
 


```
  
Watch this video to see the concepts visually. 

INSERT VIDEO (to be created)

Now back to our sampling distribution.  We have one study with 40 frogs and we are now going to use random *resampling with replacement*, to pretend that we have 1000 studies.  Like before, with our thought experiment, we will end up with data from 1000 samples of 40 frogs, but this time, the data is based on real life data, which we generated in our one frog study.  It is not as good as actually having 1000 different studies, but it is better than having only one.  

Let's remind ourselves of why we are interested in having 1000 samples... Our scientific question about the average weight of this frog species, is about the *population* of frogs, not just about our small *sample* of 40.  We are trying to *estimate* the true *population* mean weight, using only the data we have from this *sample*.  By resampling with replacement, we create 1000 "pretend" samples, so that we can create the *sampling distribution*, as before with our thought experiment.  

The sampling distribution is a *probability distribution*.  For every value on the x-axis of the graph, you can calculate the probability of a study finding that specific mean weight, or a weight difference smaller (or larger) than it, given what we know about the *population*.  Let's look at the sampling distribution from our resampling with replacement exercise. This time we will look at a density plot, rather than a histogram.  

```{r}

set.seed(3405)
plotdta <- tibble(mean_wt = rnorm(1000, mean = 34, sd = 3.5),
       mean_pop = mean(mean_wt),
       q2.5 = quantile(mean_wt, probs = 0.025),
       q97.5 = quantile(mean_wt, probs = 0.975)) 

plotdta |> 
  ggplot(aes(mean_wt))+
  geom_density(fill = themeCols$hex[1], colour = NA)+
  geom_vline(xintercept = plotdta$mean_pop, colour = "black")+
  geom_vline(xintercept = plotdta$q2.5, linetype = 2, colour = "grey70")+
  geom_vline(xintercept = plotdta$q97.5, linetype = 2, colour = "grey70")+
  annotate("text", x = plotdta$mean_pop, 
           y = 0.12, label = round(plotdta$mean_pop, 2))+
  annotate("text", x = plotdta$q2.5, 
           y = 0.12, label = round(plotdta$q2.5, 2))+
  annotate("text", x = plotdta$q97.5, 
           y = 0.12, label = round(plotdta$q97.5, 2))


```

The solid black line in the middle, indicates the mean.  The grey dotted lines on either side, indicate the 2.5th percentile and the 97.5th percentile, respectively.  

::: {.column-margin}

If you need to review what a *percentile* is, now is a good time to look that up.  

:::

In a probability distribution, 2.5% of the data is found below the 2.5th percentile.  Another way to say that is that the probability of a frog to have a weight lower than the 2.5th percentile, is 2.5%.  In the same way, the probability of a frog weighing more than the 97.5th percentile, is 2.5%.  And taking those two facts together, the probability of a frog having a weight between the 2.5th and the 97.5th percentile, is 100% minus (2.5% + 2.5% = 5%) = 95%.  This is the usefulness of the sampling distribution - it gives us an estimate of the uncertainty (or probability) around our estimate of the mean weight for this frog species.   
More formally, we think that the probability is 95% that the true *population* mean lies between the 2.5th and the 97.5th percentiles of the sampling distribution.  

```{r}
# 
# set.seed(3405)
# plotdta <- tibble(mean_wt = rnorm(1000, mean = 34, sd = 3.5),
#        mean_pop = mean(mean_wt),
#        q2.5 = quantile(mean_wt, probs = 0.025),
#        q97.5 = quantile(mean_wt, probs = 0.975)) 
# 
# plotdta |> 
#   ggplot(aes(mean_wt))+
#   geom_density(fill = themeCols$hex[1], colour = NA)+
#   geom_vline(xintercept = plotdta$mean_pop, colour = "black")+
#   geom_vline(xintercept = plotdta$q2.5, linetype = 2, colour = "grey70")+
#   geom_vline(xintercept = plotdta$q97.5, linetype = 2, colour = "grey70")+
#   annotate("text", x = plotdta$mean_pop, 
#            y = 0.12, label = round(plotdta$mean_pop, 2))+
#   annotate("text", x = plotdta$q2.5, 
#            y = 0.12, label = round(plotdta$q2.5, 2))+
#   annotate("text", x = plotdta$q97.5, 
#            y = 0.12, label = round(plotdta$q97.5, 2))


```

Now let us make the connection between *inference* and the *sampling distribution*.  Inference refers to the process by which we use information from a *sample* to describe a *population*.  We take measurements from a relatively small group of *representatives* of the population of interest (the sample), to *infer* reasonable estimates of what those measurements would be, on average, for the whole population.  

The sampling distribution that we created by resampling with replacement, using our *sample* data, is our attempt to get at what the distribution of weight might look like for the whole population.  Because we are obviously not 100% *certain* of our estimates (because we cannot measure the whole population), the sampling distribution is a *probability distribution*, that specifies the *probability* that the population estimate lies between specific values, e.g. the 2.5th and the 97.5th percentiles.  
In the next section we will take this concept further, into difference tests.  

